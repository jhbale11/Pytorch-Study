{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch로 시작하는 딥러닝 입문\n",
    "\n",
    "## 5장 : Artificial Neural Networks\n",
    "\n",
    "https://wikidocs.net/60021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 용어의 이해\n",
    "\n",
    "### 1) 머신 러닝 모델의 평가\n",
    "![](https://wikidocs.net/images/page/24987/%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\n",
    "\n",
    "실제 모델을 평가하기 위해서 데이터를 train, test, validation으로 분리하는 것이 일반적입니다. 그렇다면 굳이 validation data를 만드는 이유는 무엇일까요?\n",
    "\n",
    "validation data는 모델의 성능을 평가하기 위한 용도가 아니라 모델의 성능을 조정하기 위한 용도입니다. 더 정확히는 과적합이 일어나고 있는지 판단하거나, 하이퍼파라미터 조정을 위한 용도입니다.\n",
    "\n",
    "**하이퍼 파라미터**는 사용자가 설정할 수 있는 값으로 모델의 성능에 영향을 주는 파라미터를 말합니다.\n",
    "반면 **파라미터**는 가중치와 편향과 같이 학습을 통해 바뀌어 가는 변수를 말합니다.\n",
    "하이퍼 파라미터는 사용자가 직접 정해줄 수 있는 변수인 반면, 파라미터는 모델이 학습하는 과정에서 얻어지는 값입니다.\n",
    "\n",
    "train data로 학습시킨 모델은 validation data를 사용하여 정확도를 검증하며, hyper parameter를 tuning합니다.\n",
    "\n",
    "hyperparameter tuning이 끝났다면 test data를 사용하여 모델을 평가합니다. 모델이 train, validation data를 통해 최적화가 되었기에 새로운 데이터를 통해 평가하는 것이 바람직합니다.\n",
    "\n",
    "### 2) 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)\n",
    "**지도학습**이란 label이라는 정답과 함께 학습하는 것을 말합니다.\n",
    "**비지도학습**이란 기본적으로 target data 혹은 label이 없는 학습 방법입니다. 대표적으로 clustering이나 차원 축소와 같은 방법을 말합니다.\n",
    "**강화학습**은 어떤 환경 내에서 정의된 에이전트가 현재의 상태를 인식하여 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법입니다.\n",
    "\n",
    "### 3) 과적합(Overfitting)과 과소적합(Underfitting)\n",
    "**과적합**이란 훈련 데이터를 과하게 학습한 경우를 말합니다. 훈련데이터는 실제로 존재하는 많은 데이터의 일부에 불과한 바, 모델이 train data에 대해서만 과하게 학습할 경우 test data나 실제 데이터에 대해서는 좋은 정확도를 보이지 못할 수 있습니다.\n",
    "\n",
    "과적합 상황에서는 train data에 대해서는 오차가 낮지만, test data에 대해서는 오차가 높아지는 상황이 발생합니다.\n",
    "![](https://wikidocs.net/images/page/32012/%EC%8A%A4%ED%8C%B8_%EB%A9%94%EC%9D%BC_%EC%98%A4%EC%B0%A8.png)\n",
    "\n",
    "위의 그래프에서 X축의 epoch는 전체 train data에 대한 훈련 횟수를 의미합니다. 위의 그래프에서는 epoch가 5가 넘어가면 과적합이 발생하며 epoch가 증가할수록 test data에 대한 오차가 점차 증가하는 양상을 보입니다. \n",
    "\n",
    "이상적으로는 epoch가 증가할수록 train data에 대한 loss가 감소함과 동시에 test data에 대한 loss 역시 감소하는 것이 좋습니다. 과적합을 막기 위해 Drop out, Early Stopping과 같은 방법을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Perceptron\n",
    "\n",
    "Perceptron은 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘입니다. \n",
    "![](https://wikidocs.net/images/page/24958/perceptrin1_final.PNG)\n",
    "위의 그림과 같이 n개의 입력 x를 받아 각각 다른 가중치인 W를 가지게 되며 최종적으로 각 입력값이 가중치와 곱해져서 결과로 출력됩니다.\n",
    "\n",
    "### 1) Single-Layer Perceptron\n",
    "![](https://wikidocs.net/images/page/24958/perceptron3_final.PNG)\n",
    "\n",
    "단층 퍼셉트론은 값을 보내는 단계와 받아서 출력하는 단계, 2가지 단계로만 이루어집니다. 이 각 단계를 Layer라고 하며 두개의 층을 input layer, output layer라고 합니다.\n",
    "\n",
    "### 2) Multi-Layer Perceptron\n",
    "![](https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG)\n",
    "위와 같이 은닉층이 2개 이상인 신경망을 DNN(Deep Neural Network)이라고 합니다. 모델이 가중치를 스스로 찾아내도록 자동화시키는 과정이 머신러닝에서 말하는 **학습**이며, 앞의 회귀와 마찬가지로 **Loss Function**과 **Optimizer**이 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 역전파(Back Propagation)\n",
    "\n",
    "인공신경망이 순전파 과정을 진행하여 예측값과 실제값의 오차를 계산하였을 때 어떻게 역전파 과정에서 경사 하강법을 사용하여 가중치를 업데이트하는지 직접 계산하고자 합니다.\n",
    "\n",
    "### 1) Neural Network Overview\n",
    "![](https://wikidocs.net/images/page/37406/nn1_final.PNG)\n",
    "예제로 사용될 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가지며, 두개의 입력과 두개의 은닉층 뉴런, 두개의 출력층 뉴런을 사용합니다. 은닉층과 출력층 모두 활성화 함수로 시그모이드 함수를 사용합니다.\n",
    "\n",
    "### 2) Forward Propagation\n",
    "![](https://wikidocs.net/images/page/37406/nn2_final_final.PNG)\n",
    "\n",
    "z1과 z2는 각 입력에 해당하는 가주이와 곱해지고 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됩니다.\n",
    "\n",
    "$\n",
    "z_{1}=W_{1}x_{1} + W_{2}x_{2}=0.3 \\text{×} 0.1 + 0.25 \\text{×} 0.2= 0.08\n",
    "$\n",
    "$\n",
    "z_{2}=W_{3}x_{1} + W_{4}x_{2}=0.4 \\text{×} 0.1 + 0.35 \\text{×} 0.2= 0.11\n",
    "$\n",
    "\n",
    "시그모이드 함수가 반환하는 값은 은닉층 뉴런의 최종 출력값 h1, h2입니다.\n",
    "\n",
    "$\n",
    "h_{2}=sigmoid(z_{2}) = 0.52747230\n",
    "$\n",
    "$\n",
    "h_{2}=sigmoid(z_{2}) = 0.52747230\n",
    "$\n",
    "\n",
    "h1과 h2는 다시 출력층의 뉴런으로 향하게 되며 다시 각각의 값에 해당하는 가중치와 곱해지게 되며 다시 가중합되어 출력층 뉴런의 시그모이드 함수의 입력값이 됩니다.\n",
    "\n",
    "$\n",
    "z_{3}=W_{5}h_{1}+W_{6}h_{2} = 0.45 \\text{×} h_{1} + 0.4 \\text{×} h_{2} = 0.44498412\n",
    "$\n",
    "$\n",
    "z_{4}=W_{7}h_{1}+W_{8}h_{2} = 0.7 \\text{×} h_{1} + 0.6 \\text{×} h_{2} = 0.68047592\n",
    "$\n",
    "\n",
    "시그모이드 함수가 반환하는 값은 실제값을 예측하기 위한 값으로서 예측값이라고 부릅니다.\n",
    "\n",
    "$\n",
    "o_{1}=sigmoid(z_{3})=0.60944600\n",
    "$\n",
    "$\n",
    "o_{2}=sigmoid(z_{4})=0.66384491\n",
    "$\n",
    "\n",
    "마지막으로 예측값과 실제값의 오차를 계산하기 위해 오차 함수를 선택하여야 합니다. 오차를 계산하기 위한 Loss Function으로는 MSE를 사용합니다.\n",
    "\n",
    "$\n",
    "E_{o1}=\\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02193381\n",
    "$\n",
    "$\n",
    "E_{o2}=\\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00203809\n",
    "$\n",
    "$\n",
    "E_{total}=E_{o1}+E_{o2}=0.02397190\n",
    "$\n",
    "\n",
    "### 3) Back Propagation Step\n",
    "\n",
    "순전파가 입력층에서 출력층으로 향한다면 역전파는 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트 해갑니다. \n",
    "\n",
    "![](https://wikidocs.net/images/page/37406/nn3_final.PNG)\n",
    "이를 계산하기 위해 Chain Rule을 사용하며, 다음과 같이 풀어낼 수 있습니다.\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{5}} = \\frac{∂E_{total}}{∂o_{1}} \\text{×} \\frac{∂o_{1}}{∂z_{3}} \\text{×} \\frac{∂z_{3}}{∂W_{5}}\n",
    "$\n",
    "$\n",
    "E_{total}=\\frac{1}{2}(target_{o1}-output_{o1})^{2} + \\frac{1}{2}(target_{o2}-output_{o2})^{2}\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂o_{1}}=2 \\text{×} \\frac{1}{2}(target_{o1}-output_{o1})^{2-1} \\text{×} (-1) + 0\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{6}} = \\frac{∂E_{total}}{∂o_{1}} \\text{×} \\frac{∂o_{1}}{∂z_{3}} \\text{×} \\frac{∂z_{3}}{∂W_{6}} → W_{6}^{+}=0.38685205\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{7}} = \\frac{∂E_{total}}{∂o_{2}} \\text{×} \\frac{∂o_{2}}{∂z_{4}} \\text{×} \\frac{∂z_{4}}{∂W_{7}} → W_{7}^{+}=0.69629578\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{8}} = \\frac{∂E_{total}}{∂o_{2}} \\text{×} \\frac{∂o_{2}}{∂z_{4}} \\text{×} \\frac{∂z_{4}}{∂W_{8}} → W_{8}^{+}=0.59624247\n",
    "$\n",
    "\n",
    "이와 같은 원리로 1차 역전파에서 업데이트 된 가중치 값을 구할 수 있으며, 위와 같은 방식으로 2차 역전파에서도 다음과 같은 가중치 값을 구할 수 있다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/37406/nn4.PNG)\n",
    "\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{2}} = \\frac{∂E_{total}}{∂h_{1}} \\text{×} \\frac{∂h_{1}}{∂z_{1}} \\text{×} \\frac{∂z_{1}}{∂W_{2}}  → W_{2}^{+}=0.24919112\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{3}} = \\frac{∂E_{total}}{∂h_{2}} \\text{×} \\frac{∂h_{2}}{∂z_{2}} \\text{×} \\frac{∂z_{2}}{∂W_{3}}  → W_{3}^{+}=0.39964496\n",
    "$\n",
    "$\n",
    "\\frac{∂E_{total}}{∂W_{4}} = \\frac{∂E_{total}}{∂h_{2}} \\text{×} \\frac{∂h_{2}}{∂z_{2}} \\text{×} \\frac{∂z_{2}}{∂W_{4}} → W_{4}^{+}=0.34928991\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Pytorch로 다층 퍼센트론 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Sigmoid()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드는 입력층, 은닉층 1, 은닉층 2, 은닉층 3, 출력층을 가지는 은닉층이 3개인 인공신경망이며 그림으로 표현하면 아래와 같습니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/61010/ann.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6948983669281006\n",
      "100 0.693155825138092\n",
      "200 0.6931535601615906\n",
      "300 0.6931513547897339\n",
      "400 0.693149209022522\n",
      "500 0.6931473016738892\n",
      "600 0.6931453943252563\n",
      "700 0.6931434273719788\n",
      "800 0.6931416988372803\n",
      "900 0.6931397914886475\n",
      "1000 0.6931380033493042\n",
      "1100 0.6931361556053162\n",
      "1200 0.6931343078613281\n",
      "1300 0.6931324005126953\n",
      "1400 0.6931304931640625\n",
      "1500 0.6931284666061401\n",
      "1600 0.6931264400482178\n",
      "1700 0.6931242942810059\n",
      "1800 0.6931220889091492\n",
      "1900 0.6931197047233582\n",
      "2000 0.6931171417236328\n",
      "2100 0.6931145191192627\n",
      "2200 0.6931115984916687\n",
      "2300 0.6931084990501404\n",
      "2400 0.6931051015853882\n",
      "2500 0.6931014657020569\n",
      "2600 0.6930974721908569\n",
      "2700 0.6930930018424988\n",
      "2800 0.6930879950523376\n",
      "2900 0.6930825114250183\n",
      "3000 0.6930763721466064\n",
      "3100 0.6930692195892334\n",
      "3200 0.6930612325668335\n",
      "3300 0.6930519342422485\n",
      "3400 0.6930410861968994\n",
      "3500 0.6930283904075623\n",
      "3600 0.6930132508277893\n",
      "3700 0.6929951310157776\n",
      "3800 0.6929728984832764\n",
      "3900 0.6929453015327454\n",
      "4000 0.6929103136062622\n",
      "4100 0.6928649544715881\n",
      "4200 0.6928046941757202\n",
      "4300 0.692721962928772\n",
      "4400 0.692604124546051\n",
      "4500 0.6924278736114502\n",
      "4600 0.6921480298042297\n",
      "4700 0.6916664838790894\n",
      "4800 0.6907395124435425\n",
      "4900 0.6886203289031982\n",
      "5000 0.6820818185806274\n",
      "5100 0.6472527980804443\n",
      "5200 0.44994843006134033\n",
      "5300 0.041418157517910004\n",
      "5400 0.009738397784531116\n",
      "5500 0.005035833455622196\n",
      "5600 0.0032971729524433613\n",
      "5700 0.0024168307427316904\n",
      "5800 0.0018922279123216867\n",
      "5900 0.0015467405319213867\n",
      "6000 0.0013033139985054731\n",
      "6100 0.0011231712996959686\n",
      "6200 0.0009848718764260411\n",
      "6300 0.0008755262824706733\n",
      "6400 0.000787071418017149\n",
      "6500 0.0007141333189792931\n",
      "6600 0.0006530550308525562\n",
      "6700 0.0006011504447087646\n",
      "6800 0.0005565693136304617\n",
      "6900 0.0005178644205443561\n",
      "7000 0.00048400633386336267\n",
      "7100 0.0004541150410659611\n",
      "7200 0.00042753416346386075\n",
      "7300 0.0004037863982375711\n",
      "7400 0.0003824243613053113\n",
      "7500 0.0003631050349213183\n",
      "7600 0.00034560466883704066\n",
      "7700 0.00032958033261820674\n",
      "7800 0.00031495740404352546\n",
      "7900 0.00030152720864862204\n",
      "8000 0.0002891554613597691\n",
      "8100 0.0002777229528874159\n",
      "8200 0.00026711029931902885\n",
      "8300 0.000257257983321324\n",
      "8400 0.0002480764815118164\n",
      "8500 0.00023949125898070633\n",
      "8600 0.0002314575540367514\n",
      "8700 0.0002239307068521157\n",
      "8800 0.0002168510400224477\n",
      "8900 0.00021021856809966266\n",
      "9000 0.00020391402358654886\n",
      "9100 0.0001979821245186031\n",
      "9200 0.00019239308312535286\n",
      "9300 0.00018708722200244665\n",
      "9400 0.00018206457025371492\n",
      "9500 0.00017725062207318842\n",
      "9600 0.00017273474077228457\n",
      "9700 0.00016841263277456164\n",
      "9800 0.00016429921379312873\n",
      "9900 0.00016036465240176767\n",
      "10000 0.0001565940328873694\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    # forward 연산\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100의 배수에 해당되는 에포크마다 비용을 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[1.11739784e-04]\n",
      " [9.99828696e-01]\n",
      " [9.99842167e-01]\n",
      " [1.85383164e-04]]\n",
      "모델의 예측값(Predicted):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "실제값(Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제값은 0,1,1,0이며, 예측값도 0,1,1,0으로 문제를 잘 해결했음을 알 수 있습니다!\n",
    "\n",
    "5-2장에서는 이어서 Activation Function과 Overfitting, Gradent Vanishing에 대해 학습하겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
