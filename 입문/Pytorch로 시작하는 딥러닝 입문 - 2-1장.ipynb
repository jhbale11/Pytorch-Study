{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ade7c8",
   "metadata": {},
   "source": [
    "# PyTorch로 시작하는 딥러닝 입문\n",
    "## 2장 : Linear Regression\n",
    "https://wikidocs.net/book/2788"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a178c2e",
   "metadata": {},
   "source": [
    "# 2-1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713e877",
   "metadata": {},
   "source": [
    "## train dataset\n",
    "\n",
    "- 모델을 학습시키기 위한 데이터는 텐서의 형태를 가지고 있어야 합니다.\n",
    "- 입력과 출력을 각각 다른 텐서에 저장할 필요가 있습니다.\n",
    "- x_train은 학습시킬 input, y_train은 학습시킬 output 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f09e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17f0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe5eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628b0af",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "- H(x) = W * x + b\n",
    "- w : weight 가중치\n",
    "- b : bias 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197da830",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "- Cost Function = Loss Function = Error Function\n",
    "- 가설과 실제값의 오차를 최소로 만들어주는 함수\n",
    "\n",
    "\n",
    "## Optimizer\n",
    "- 비용함수의 값을 최소로 만들어주는 W와 b를 찾는 방법\n",
    "- 최적화 알고리즘\n",
    "- 옵티마이저를 통해 적절한 W와 b를 찾아내는 과정을 머신러닝에서 '학습'이라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca4a05",
   "metadata": {},
   "source": [
    "## 1. Basic Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c616b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75ba0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc1900b0370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f92bc5",
   "metadata": {},
   "source": [
    "## 2. Variable Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9531edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c43caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ad11f",
   "metadata": {},
   "source": [
    "# 3. Weight와 Bias의 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56ff897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "# 가중치 W를 출력\n",
    "print(W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9474bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d977cc",
   "metadata": {},
   "source": [
    "# 4. Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "542bd08a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41944d52",
   "metadata": {},
   "source": [
    "# 5. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595286e",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>c</mi>\n",
    "  <mi>o</mi>\n",
    "  <mi>s</mi>\n",
    "  <mi>t</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>W</mi>\n",
    "  <mo>,</mo>\n",
    "  <mi>b</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mn>1</mn>\n",
    "    <mi>n</mi>\n",
    "  </mfrac>\n",
    "  <munderover>\n",
    "    <mo data-mjx-texclass=\"OP\">&#x2211;</mo>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi>i</mi>\n",
    "      <mo>=</mo>\n",
    "      <mn>1</mn>\n",
    "    </mrow>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi>n</mi>\n",
    "    </mrow>\n",
    "  </munderover>\n",
    "  <msup>\n",
    "    <mrow data-mjx-texclass=\"INNER\">\n",
    "      <mo data-mjx-texclass=\"OPEN\">[</mo>\n",
    "      <msup>\n",
    "        <mi>y</mi>\n",
    "        <mrow data-mjx-texclass=\"ORD\">\n",
    "          <mo stretchy=\"false\">(</mo>\n",
    "          <mi>i</mi>\n",
    "          <mo stretchy=\"false\">)</mo>\n",
    "        </mrow>\n",
    "      </msup>\n",
    "      <mo>&#x2212;</mo>\n",
    "      <mi>H</mi>\n",
    "      <mo stretchy=\"false\">(</mo>\n",
    "      <msup>\n",
    "        <mi>x</mi>\n",
    "        <mrow data-mjx-texclass=\"ORD\">\n",
    "          <mo stretchy=\"false\">(</mo>\n",
    "          <mi>i</mi>\n",
    "          <mo stretchy=\"false\">)</mo>\n",
    "        </mrow>\n",
    "      </msup>\n",
    "      <mo stretchy=\"false\">)</mo>\n",
    "      <mo data-mjx-texclass=\"CLOSE\">]</mo>\n",
    "    </mrow>\n",
    "    <mn>2</mn>\n",
    "  </msup>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1012752e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0fba3",
   "metadata": {},
   "source": [
    "# 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e6fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a581ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "# W와 b를 업데이트\n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54378361",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ff15dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71e8ed",
   "metadata": {},
   "source": [
    "#### Epoch는 전체 훈련 데이터가 학습에 한번 사용된 주기를 의미합니다.\n",
    "#### 최종 훈련 결과를 보면 최적의 기울기 W는 2에 가깝고 편향 b는 0에 가깝습니다.\n",
    "x_train data와 y_train data가 기울기 2의 선형 관계를 가지고 있는 바 정답에 근사했다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f947069",
   "metadata": {},
   "source": [
    "# 2-2 Auto grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb88ded",
   "metadata": {},
   "source": [
    "값이 2인 임의의 스칼라 텐서 w를 선언합니다. 이때 required_grad를 True로 설정합니다. 이는 이 텐서에 대한 기울기를 저장하겠다는 의미입니다. 뒤에서 보겠지만, 이렇게 하면 w.grad에 w에 대한 미분값이 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58890d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6afcf2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = w**2\n",
    "z = 2*y + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02454b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666bc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb7caa",
   "metadata": {},
   "source": [
    "# 2-3 Multivariable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff746ca",
   "metadata": {},
   "source": [
    "## Data Definition\n",
    "훈련데이터는 다음과 같습니다. 3개의 독립변수를 가지고 최종 점수를 예측하는 모델입니다.\n",
    "![](https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\n",
    "\n",
    "이를 수식으로 표현하면 다음과 같습니다.\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>H</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>w</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>1</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>1</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>+</mo>\n",
    "  <msub>\n",
    "    <mi>w</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>2</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>2</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>+</mo>\n",
    "  <msub>\n",
    "    <mi>w</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>3</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mn>3</mn>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>+</mo>\n",
    "  <mi>b</mi>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10cbb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330538fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b583787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산 : MSE\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b71b0f",
   "metadata": {},
   "source": [
    "## 2-4 nn.Module로 구현하는 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c93b1e",
   "metadata": {},
   "source": [
    "## 단일 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ecea79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b12cdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc1900b0370>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d74b7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c0be8",
   "metadata": {},
   "source": [
    "nn.Linear()은 입력의 차원, 출력의 차원을 인수로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7689e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "model = nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bea37",
   "metadata": {},
   "source": [
    "위의 예제에서 하나의 입력 x에 대해 하나의 출력 y를 가지므로, 입력 차원과 출력 차원 모두 1을 인수로 사용하였습니다. model에는 가중치 W와 편향 b가 저장되어 있습니다.\n",
    "\n",
    "이를 parameters()로 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78df579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f8ad18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be979ec9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497eca7f",
   "metadata": {},
   "source": [
    "## 다중 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5502248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81dcc32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc1900b0370>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d44e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed28f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c13645db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfcd4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40e595c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31667.597656\n",
      "Epoch  100/2000 Cost: 0.225988\n",
      "Epoch  200/2000 Cost: 0.223910\n",
      "Epoch  300/2000 Cost: 0.221930\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218270\n",
      "Epoch  600/2000 Cost: 0.216571\n",
      "Epoch  700/2000 Cost: 0.214955\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211949\n",
      "Epoch 1000/2000 Cost: 0.210558\n",
      "Epoch 1100/2000 Cost: 0.209237\n",
      "Epoch 1200/2000 Cost: 0.207971\n",
      "Epoch 1300/2000 Cost: 0.206764\n",
      "Epoch 1400/2000 Cost: 0.205616\n",
      "Epoch 1500/2000 Cost: 0.204527\n",
      "Epoch 1600/2000 Cost: 0.203479\n",
      "Epoch 1700/2000 Cost: 0.202487\n",
      "Epoch 1800/2000 Cost: 0.201542\n",
      "Epoch 1900/2000 Cost: 0.200638\n",
      "Epoch 2000/2000 Cost: 0.199769\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c91e929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f871afb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f9470",
   "metadata": {},
   "source": [
    "결과를 보면,\n",
    "- W1 : 0.9778\n",
    "- W2 :0.4539\n",
    "- W3 : 0.5768\n",
    "- b : 0.2802\n",
    "위와 같이 최적화 되어 있음을 확인할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
