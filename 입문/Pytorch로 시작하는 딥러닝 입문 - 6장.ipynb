{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch로 시작하는 딥러닝 입문\n",
    "\n",
    "## 6장 : Convolutional Neural Networks\n",
    "\n",
    "### References\n",
    "\n",
    "https://wikidocs.net/62306\n",
    "\n",
    "http://taewan.kim/post/cnn/\n",
    "\n",
    "https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "\n",
    "https://excelsior-cjh.tistory.com/152\n",
    "\n",
    "https://buomsoo-kim.github.io/keras/2018/04/28/Easy-deep-learning-with-Keras-7.md/\n",
    "\n",
    "https://becominghuman.ai/not-just-introduction-to-convolutional-neural-networks-part-1-56a36b938592\n",
    "\n",
    "https://brohrer.github.io/how_convolutional_neural_networks_work.html\n",
    "\n",
    "https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "\n",
    "https://www.kdnuggets.com/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html\n",
    "\n",
    "https://www.slideshare.net/leeseungeun/cnn-vgg-72164295"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN은 이미지 처리에 탁월한 성능을 보이는 신경망입니다. Convolutional Layer와 Pooling Layer로 구성되어 있는 CNN은 CONV -> ReLu -> POOL을 거쳐 연산됩니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/62306/convpooling.PNG)\n",
    "\n",
    "위의 그림에서 볼 수 있는 것처럼 CONV->ReLu->CONV->ReLu->POOL의 Layer를 세번 반복하며 최종적으로 FC Layer로 들어가 이미지 분류를 수행합니다.\n",
    "\n",
    "## 1. CNN의 대두\n",
    "\n",
    "CNN은 이미지 처리에 좋은 성능을 보이는 신경망입니다. 이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다. 이미지 처리를 하기 위해서 앞서 배운 다층 퍼셉트론을 사용할 수는 있지만 한계가 있었습니다.\n",
    "\n",
    "예를 들어 알파벳 손글씨 분류하는 문제가 있다고 가정합시다. 아래의 그림은 알파벳 Y를 비교적 정자로 쓴 글씨와 다소 휘갈겨 쓴 글씨 두개를 2차원 텐서인 행렬로 표현한 것입니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv0.png)\n",
    "\n",
    "사람이 보기에는 두 그림 모두 알바벳 Y로 손쉽게 판단이 가능하지만 기계가 보기에는 각 픽셀마다 값이 거의 상이하므로 완전히 다른 값을 가진 입력입니다. 그러나 이미지라는 것이 위와 같은 예시처럼 대상이 휘어지거나, 이동되었거나, 방향이 다르다는 등 다양한 변형이 존재하기에 다층 퍼셉트론은 몇가지 픽셀만 값이 달라져도 민감하게 예측에 영향을 받는다는 단점이 있습니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv1.png)\n",
    "\n",
    "구체적으로 보면, 이미지를 분류한다고 하면 이미지를 1차원 텐서인 벡터로 변환하고 입력층으로 사용해야 하기에 원래 어떤 이미지였느지 알아보기가 어렵습니다. 따라서 이미지의 공간적인 구조 정보를 보존하면서 학습할 수 있는 방법이 필요해졌고 이를 위해 사용하는 것이 바로 CNN입니다.\n",
    "\n",
    "## 2. Channel\n",
    "\n",
    "기계는 글자나 이미지보다 숫자, 다시 말해 텐서를 더 잘 처리할 수 있습니다. 이미지는 **(높이, 너비, 채널)** 이라는 3차원 텐서입니다. 여기서 높이는 이미지의 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지의 경우 채널이 1이며, 각 픽셀은 0부터 255까지의 값을 가집니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv3.png)\n",
    "\n",
    "컬러 이미지는 위와 같이 R, G, B 값을 각각 가지는 3차원 텐서입니다.\n",
    "\n",
    "## 3. Convolution Operation\n",
    "\n",
    "합성곱층은 합성곱 연산을 통해 이미지의 특징을 추출하는 역할을 합니다. Convolution은 Kernel 또는 Filter라는 nxm 크기의 높이와 너비의 이미지를 처음부터 끝까지 겹치며 훑으면서 nxm 크기의 겹쳐지는 부분의 각 이미지와 커널의 원소의 값을 곱해서 모두 더한 값을 출력으로 하는 것을 말합니다. \n",
    "\n",
    "- kernel은 일반적으로 3x3, 5x5 를 사용합니다.\n",
    "\n",
    "### 1) Step1\n",
    "![](https://wikidocs.net/images/page/64066/conv4.png)\n",
    "\n",
    "### 2) Step2\n",
    "![](https://wikidocs.net/images/page/64066/conv5.png)\n",
    "\n",
    "### 3) Step2\n",
    "![](https://wikidocs.net/images/page/64066/conv6.png)\n",
    "\n",
    "위와 같은 Step을 9번 마쳤다고 가정하였을 때, 최종 결과는 다음과 같습니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv8.png)\n",
    "\n",
    "위와 같이 입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과를 **특성 맵(Feature Map)** 이라고 합니다. 위의 예제에서는 커널의 크기가 3x3이었으나, 커널의 크기는 사용자가 정할 수 있으며, 위의 예제에서는 커널의 이동 범위가 1칸이었으나 이 또한 사용자가 설정할 수 있습니다.\n",
    "\n",
    "이러한 이동 범위를 **Stride**라고 합니다.\n",
    "\n",
    "## 4. Padding\n",
    "![](https://wikidocs.net/images/page/64066/conv10.png)\n",
    "\n",
    "위의 예시에서 커널로 합성곱 연산을 하였을 때 그 결과로 얻은 Feature Map은 입력보다 크기가 작아진다는 특징이 있습니다. 만약 합성곱 층을 여러개 쌓았다면 최종적으로 얻은 특성 맵은 초기 입력보다 매우 작아진 상태가 되어버립니다. 합성곱 연산 이후에도 Feature Map의 크기가 일정하게 유지되도록 하고 싶다면 **Padding**을 이용하면 됩니다.\n",
    "\n",
    "Padding은 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가해주는 것을 말합니다. 주로 값을 0으로 채우는 **Zero Padding**을 사용합니다.\n",
    "\n",
    "\n",
    "## 5. Weight & Bias\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv13.png)\n",
    "\n",
    "CNN에서는 Feature Map을 얻기 위해서 동일한 kernel로 이미지 전체를 훑으며 합성곱 연산을 진행합니다. 결국 이미지 전체를 훑으면서 사용되는 Weight은 다층 퍼셉트론을 사용할 때 9개인 것에 비해 4개 뿐입니다. 결국 CNN은 다층 퍼셉트론을 사용할 때보다 훨씬 적은 수의 가중치를 사용하며 공간적 구조 정보를 보존한다는 특징이 있습니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv14.png)\n",
    "\n",
    "합성곱 신경망에 Bias는 당연히 추가할 수 있습니다. 만약 Bias를 사용한다면 Kernel을 적용한 뒤 Feature Map에 더해집니다. Bias는 하나의 값만 존재하며 kernel이 적용된 결과의 모든 원소에 더해집니다.\n",
    "\n",
    "## 6. 3D Tensor Convolution - 1\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv15.png)\n",
    "\n",
    "위 그림은 3개의 채널을 가진 입력 데이터와 3개의 채널을 가진 커널의 합성곱 연산을 보여줍니다. 커널의 각 채널끼리의 크기는 같아야 합니다. 각 채널 간 합성곱 연산을 마치고, 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵을 만듭니다. 주의할 점은 위의 연산에서 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 점입니다.\n",
    "\n",
    "위 그림은 높이 3, 너비 3, 채널 3의 입력이 높이 2, 너비 2, 채널 3의 커널과 합성곱 연산을 하여 높이 2, 너비 2, 채널 1의 Feature Map을 얻는다는 의미입니다. 합성곱 연산의 결과로 얻은 특성맵의 채널 차원은 RGB 채널 등과 같은 컬러의 의미를 담고 있지는 않습니다.\n",
    "\n",
    "## 7. 3D Tensor Convolution - 2\n",
    "\n",
    "다음은 3차원 텐서의 합성곱 연산을 보여줍니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv16_final.png)\n",
    "\n",
    "높이 Ih, 너비 Iw 채널 Ci의 입력 데이터는 동일한 채널수를 가지는 높이 Kh, 너비 Kw의 커널과 합성곱 연산을 하여 높이 Oh, 너비 Ow 채널 1의 특성맵을 얻습니다\n",
    "\n",
    "![](https://wikidocs.net/images/page/64066/conv17_final_final.PNG)\n",
    "\n",
    "합성곱 연산에서 다수의 커널을 사용할 경우, 사용한 커널 수는 합성곱 연산의 결과로 나오는 특성 맵의 채널 수가 됩니다.\n",
    "\n",
    "## 8. Pooling\n",
    "\n",
    "일반적으로는 CONV->ReLu 다음에는 POOL Layer를 추가하는 것이 일반적입니다. POOL Layer에서는 특성 맵을 다운 샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어집니다. 풀링 연산에는 일반적으로 최대값을 선택하는 Max Pooling과 평균값을 출력하는 Average Pooling이 사용됩니다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/62306/maxpooling.PNG)\n",
    "\n",
    "위의 그림은 Max Pooling을 나타냅니다. 풀링 연산도 합성곱 연산과 마찬가지로 Kernel과 Stride의 개념을 가지며 위의 그림과 같이 Stride가 2일 때, 2x2 크기 Kernel로 Max Pooling 연산을 했을 때 특성 맵이 절반의 크기로 다운 샘플링 되는 것을 보여줍니다. Max Pooling은 커널과 겹치는 영역 안에서 최대값을 추출하는 방식으로 다운샘플링합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 파라미터 설정\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdae41555884f188b481700e4c6b10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ebb4045cd4bebb1101d49b321dafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb34c88fc6cf4a98bbbca584894d0666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025e54b453c74d46b071128807137203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
    "                          transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
    "                         transform=transforms.ToTensor(), # 텐서로 변환\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 첫번째층\n",
    "        # ImgIn shape=(?, 28, 28, 1)\n",
    "        #    Conv     -> (?, 28, 28, 32)\n",
    "        #    Pool     -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 두번째층\n",
    "        # ImgIn shape=(?, 14, 14, 32)\n",
    "        #    Conv      ->(?, 14, 14, 64)\n",
    "        #    Pool      ->(?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss Function & Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 600\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.22562705\n",
      "[Epoch:    2] cost = 0.0630102158\n",
      "[Epoch:    3] cost = 0.0463018902\n",
      "[Epoch:    4] cost = 0.0374556705\n",
      "[Epoch:    5] cost = 0.0313643441\n",
      "[Epoch:    6] cost = 0.0259706657\n",
      "[Epoch:    7] cost = 0.021776244\n",
      "[Epoch:    8] cost = 0.0182836782\n",
      "[Epoch:    9] cost = 0.0161042772\n",
      "[Epoch:   10] cost = 0.0133900568\n",
      "[Epoch:   11] cost = 0.00968091376\n",
      "[Epoch:   12] cost = 0.00999730639\n",
      "[Epoch:   13] cost = 0.00827130955\n",
      "[Epoch:   14] cost = 0.00604190631\n",
      "[Epoch:   15] cost = 0.00732163386\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y느 ㄴ레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\82104\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9869999885559082\n"
     ]
    }
   ],
   "source": [
    "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.6% 정도의 정확도를 확인할 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
